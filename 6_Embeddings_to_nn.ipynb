{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Libraries \ud83d\udcda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os, random, ast, json, math, gc, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import optuna, warnings, shutil, tempfile\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load data \ud83d\udce5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diagnoses_df = pd.read_csv(\"data/ground_truth_df.csv\")\n",
        "for col in [\"Codigos_diagnosticos\", \"Diagnosticos_estandar\"]:\n",
        "    diagnoses_df[col] = diagnoses_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "diagnoses_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pre-process and splits \ud83e\uddf9\u2702\ufe0f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(diagnoses_df[\"Diagnosticos_estandar\"])\n",
        "# Add the prefix required by e5-Large\n",
        "texts_prefixed = [\"query: \" + t for t in diagnoses_df[\"Descripcion_diagnosticos\"].tolist()]\n",
        "\n",
        "# Random splits on multilabel data, ensuring that each label's distribution is preserved across training and test sets.\n",
        "msss = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1, test_size=0.30, random_state=SEED\n",
        ")\n",
        "X = np.array(texts_prefixed)\n",
        "\n",
        "for train_idx, tmp_idx in msss.split(np.zeros(len(X)), y):\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_tmp,   y_tmp   = X[tmp_idx],   y[tmp_idx]\n",
        "\n",
        "# 50-50 over the 30 % left \u21d2 15 %/15 %\n",
        "msss_val = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1, test_size=0.50, random_state=SEED\n",
        ")\n",
        "for val_idx, test_idx in msss_val.split(np.zeros(len(X_tmp)), y_tmp):\n",
        "    X_val,  y_val  = X_tmp[val_idx],  y_tmp[val_idx]\n",
        "    X_test, y_test = X_tmp[test_idx], y_tmp[test_idx]\n",
        "\n",
        "# Convert to lists for compatibility with SentenceTransformer\n",
        "X_train, X_val, X_test = map(lambda a: a.tolist(), [X_train, X_val, X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check that each label maintains its ratio approx.\n",
        "train_ratio = y_train.sum(axis=0) / y.sum(axis=0)\n",
        "val_ratio   = y_val.sum(axis=0)   / y.sum(axis=0)\n",
        "test_ratio  = y_test.sum(axis=0)  / y.sum(axis=0)\n",
        "\n",
        "print(f\"train: {np.round(train_ratio.mean(), 3)}, val: {np.round(val_ratio.mean(), 3)}, test: {np.round(test_ratio.mean(), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset y DataLoaders \u2699\ufe0f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DiagnosisDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        enc = tokenizer(\n",
        "            texts, padding=True, truncation=True, max_length=max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = enc[\"input_ids\"]\n",
        "        self.attn_mask = enc[\"attention_mask\"]\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self): return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attn_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline single model \ud83c\udf31\n",
        "\n",
        "-------------------------------------------------\n",
        "We freeze the encoder and train ONLY one linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "EPOCHS_BASELINE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "for p in base_model.parameters(): p.requires_grad = False  # \u2744\ufe0f freeze embeddings\n",
        "\n",
        "class BaselineClassifier(nn.Module):\n",
        "    def __init__(self, encoder, hidden_dim=0, num_labels=None):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        encoder_dim = encoder.config.hidden_size\n",
        "        if hidden_dim > 0:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(encoder_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, num_labels)\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Linear(encoder_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            pooled = outputs.last_hidden_state[:, 0, :]  # CLS\n",
        "        return self.classifier(pooled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = DiagnosisDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "val_ds   = DiagnosisDataset(X_val,   y_val,   tokenizer, MAX_LEN)\n",
        "test_ds = DiagnosisDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE*2)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline = BaselineClassifier(base_model, hidden_dim=0, num_labels=y.shape[1]).to(device)\n",
        "optimizer = torch.optim.AdamW(baseline.classifier.parameters(), lr=LR)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_loop(model, data_loader, criterion, optimizer, scheduler=None, train=False):\n",
        "    model.train() if train else model.eval()\n",
        "    losses, logits_list, labels_list = [], [], []\n",
        "    for batch in tqdm(data_loader, desc=f\"Batches ({'train' if train else 'eval'})\"):\n",
        "        input_ids  = batch[\"input_ids\"].to(device)\n",
        "        attn_mask  = batch[\"attention_mask\"].to(device)\n",
        "        labels     = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attn_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        if train:\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            if scheduler: scheduler.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        logits_list.append(torch.sigmoid(outputs).detach().cpu())\n",
        "        labels_list.append(labels.detach().cpu())\n",
        "    y_pred = (torch.vstack(logits_list) > 0.5).int().numpy()\n",
        "    y_true = torch.vstack(labels_list).numpy()\n",
        "    return (np.mean(losses),\n",
        "            f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "            precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "            recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "for epoch in range(1, EPOCHS_BASELINE+1):\n",
        "    print(f\"\ud83d\udd39 Epoch {epoch:02d} / {EPOCHS_BASELINE}\")\n",
        "    train_loss, train_f1, _, _ = epoch_loop(baseline, train_dl, criterion, optimizer, train=True)\n",
        "    val_loss, val_f1, _, _     = epoch_loop(baseline, val_dl, criterion, optimizer, train=False)\n",
        "    print(f\"Epoch [{epoch:02d}]  train f1={train_f1:.4f} | val f1={val_f1:.4f}\")\n",
        "    best_f1 = max(best_f1, val_f1)\n",
        "\n",
        "_, f1_train, precision_train, recall_train = epoch_loop(baseline, train_dl, criterion, optimizer, train=False)\n",
        "_, f1_val, precision_val, recall_val = epoch_loop(baseline, val_dl, criterion, optimizer, train=False)\n",
        "_, f1_test, precision_test, recall_test = epoch_loop(baseline, test_dl, criterion, optimizer, train=False)\n",
        "print(f\"\ud83d\udd39 TRAIN: F1 {f1_train:.4f} / Precision {precision_train:.4f} / Recall {recall_train:.4f}\")\n",
        "print(f\"\ud83d\udd39 VAL:   F1 {f1_val:.4f} / Precision {precision_val:.4f} / Recall {recall_val:.4f}\")\n",
        "print(f\"\ud83d\udd39 TEST:  F1 {f1_test:.4f} / Precision {precision_test:.4f} / Recall {recall_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the baseline model\n",
        "torch.save({\n",
        "    \"model_state\": baseline.state_dict(),\n",
        "    \"tokenizer\": MODEL_NAME,\n",
        "    \"mlb_classes\": mlb.classes_.tolist(),\n",
        "    \"params\": {\n",
        "        \"hidden_dim\": 0,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"lr\": LR,\n",
        "        \"epochs\": EPOCHS_BASELINE\n",
        "    }\n",
        "}, \"models/baseline_embeddings_to_nn.pt\")\n",
        "print(\"\ud83d\udce6 Baseline model saved as models/baseline_embeddings_to_nn.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Search with Optuna \ud83d\ude80\n",
        "-------------------------------------------------\n",
        "We search for LR, hidden_dim, max_len, and batch_size.  \n",
        "We use pruning to stop underperforming experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE_OPTUNA   = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # --- Hyperparameters to explore ---\n",
        "    lr          = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
        "    hidden_dim  = trial.suggest_int(\"hidden_dim\", 0, 2048, step=64)\n",
        "    max_len     = trial.suggest_categorical(\"max_len\", [128, 192, 256])\n",
        "    epochs      = trial.suggest_int(\"epochs\", 3, 25)\n",
        "\n",
        "    print(f\"\ud83d\udd0d Trial {trial.number} | lr={lr:.6f}, hidden_dim={hidden_dim}, max_len={max_len}, epochs={epochs}\")\n",
        "\n",
        "    # ---- Dataset - (re-tokenizo for the specific max_len) ----\n",
        "    token_tmp = tokenizer\n",
        "    train_ds_t = DiagnosisDataset(X_train, y_train, token_tmp, max_len)\n",
        "    val_ds_t   = DiagnosisDataset(X_val,   y_val,   token_tmp, max_len)\n",
        "    train_dl_t = DataLoader(train_ds_t, batch_size=BATCH_SIZE_OPTUNA, shuffle=True)\n",
        "    val_dl_t   = DataLoader(val_ds_t,   batch_size=BATCH_SIZE_OPTUNA*2)\n",
        "\n",
        "    base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "    for p in base_model.parameters(): p.requires_grad = False\n",
        "    model_t = BaselineClassifier(base_model, hidden_dim=hidden_dim, num_labels=y.shape[1]).to(device)\n",
        "\n",
        "    optimizer_t = torch.optim.AdamW(model_t.parameters(), lr=lr)\n",
        "    criterion_t = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # ---- Trained for a few epochs with pruning ----\n",
        "    for epoch in range(epochs):\n",
        "        _, _, _, _ = epoch_loop(model_t, train_dl_t, criterion_t, optimizer_t, train=True)\n",
        "        _, val_f1_epoch, _, _ = epoch_loop(model_t, val_dl_t, criterion_t, optimizer_t, train=False)\n",
        "        trial.report(val_f1_epoch, epoch)\n",
        "\n",
        "        if trial.should_prune(): raise optuna.TrialPruned()\n",
        "    \n",
        "    return val_f1_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"diagnosis_cls_baseline\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "    storage=\"sqlite:///optuna/Embeddings_to_nn.db\", load_if_exists=True\n",
        ")\n",
        "\n",
        "TOTAL_TRIALS = 25\n",
        "remaining_trials = max(TOTAL_TRIALS - len(study.trials), 0)\n",
        "study.optimize(objective, n_trials=remaining_trials, n_jobs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83c\udfc6 Best configuration:\", study.best_params)\n",
        "print(\"\ud83d\udd1d Best F1 val:\", study.best_value)\n",
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Retrain the model with the best configuration \ud83d\udd04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1) Retrieve the best search space ---\n",
        "\n",
        "best_params = study.best_params\n",
        "print(best_params)\n",
        "\n",
        "BEST_LR        = best_params[\"lr\"]\n",
        "BEST_HIDDEN    = best_params[\"hidden_dim\"]\n",
        "BEST_MAX_LEN   = best_params[\"max_len\"]\n",
        "BEST_EPOCHS    = best_params[\"epochs\"]\n",
        "\n",
        "# --- 2) Dataset: train + val (80 %) ---\n",
        "token_tmp = tokenizer\n",
        "train_ds_t = DiagnosisDataset(X_train, y_train, token_tmp, BEST_MAX_LEN)\n",
        "val_ds_t   = DiagnosisDataset(X_val,   y_val,   token_tmp, BEST_MAX_LEN)\n",
        "test_ds_t  = DiagnosisDataset(X_test, y_test, token_tmp, BEST_MAX_LEN)\n",
        "train_dl_t = DataLoader(train_ds_t, batch_size=BATCH_SIZE_OPTUNA, shuffle=True)\n",
        "val_dl_t   = DataLoader(val_ds_t,   batch_size=BATCH_SIZE_OPTUNA*2)\n",
        "test_dl_t  = DataLoader(test_ds_t,  batch_size=BATCH_SIZE_OPTUNA*2)\n",
        "\n",
        "# --- 3) Final model ---\n",
        "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "for p in base_model.parameters(): p.requires_grad = False\n",
        "model_t = BaselineClassifier(base_model, hidden_dim=BEST_HIDDEN, num_labels=y.shape[1]).to(device)\n",
        "\n",
        "optimizer_t = torch.optim.AdamW(model_t.parameters(), lr=BEST_LR)\n",
        "criterion_t = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# --- 4) Training + early-stopping ---\n",
        "for epoch in range(BEST_EPOCHS):\n",
        "    print(f\"\ud83d\udd39 Epoch {epoch+1:02d} / {BEST_EPOCHS}\")\n",
        "    _, train_f1_epoch, _, _ = epoch_loop(model_t, train_dl_t, criterion_t, optimizer_t, train=True)\n",
        "    _, val_f1_epoch, _, _ = epoch_loop(model_t, val_dl_t, criterion_t, optimizer_t, train=False)\n",
        "    \n",
        "\n",
        "_, f1_train, precision_train, recall_train = epoch_loop(model_t, train_dl_t, criterion_t, optimizer_t, train=False)\n",
        "_, f1_val, precision_val, recall_val = epoch_loop(model_t, val_dl_t, criterion_t, optimizer_t, train=False)\n",
        "_, f1_test, precision_test, recall_test = epoch_loop(model_t, test_dl_t, criterion_t, optimizer_t, train=False)\n",
        "print(f\"\ud83d\udd39 TRAIN: F1 {f1_train:.4f} / Precision {precision_train:.4f} / Recall {recall_train:.4f}\")\n",
        "print(f\"\ud83d\udd39 VAL:   F1 {f1_val:.4f} / Precision {precision_val:.4f} / Recall {recall_val:.4f}\")\n",
        "print(f\"\ud83d\udd39 TEST:  F1 {f1_test:.4f} / Precision {precision_test:.4f} / Recall {recall_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model with the best hyperparameters found by Optuna\n",
        "torch.save({\n",
        "    \"model_state\": model_t.state_dict(),\n",
        "    \"tokenizer\": MODEL_NAME,\n",
        "    \"mlb_classes\": mlb.classes_.tolist(),\n",
        "    \"params\": best_params\n",
        "}, \"models/optimized_embeddings_to_nn.pt\")\n",
        "print(\"\ud83d\udce6 Optuna best model saved as optimized_embeddings_to_nn\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
