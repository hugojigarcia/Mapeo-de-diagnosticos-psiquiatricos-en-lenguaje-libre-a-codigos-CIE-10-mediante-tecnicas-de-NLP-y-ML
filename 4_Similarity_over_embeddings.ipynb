{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Libraries \ud83d\udcda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load data \ud83d\udce5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diagnoses_df = pd.read_csv(\"data/ground_truth_df.csv\")\n",
        "diagnoses_df['Codigos_diagnosticos'] = diagnoses_df['Codigos_diagnosticos'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "diagnoses_df['Diagnosticos_estandar'] = diagnoses_df['Diagnosticos_estandar'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "diagnoses_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Similarity over embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index = 1\n",
        "diagnosis_description = diagnoses_df[\"Descripcion_diagnosticos_limpio\"][index]\n",
        "true_diagnosis_standard = diagnoses_df[\"Diagnosticos_estandar\"][index]\n",
        "\n",
        "print(f\"Diagnosis description: {diagnosis_description}\")\n",
        "print(f\"Diagnosis codes: {diagnoses_df['Codigos_diagnosticos'][index]}\")\n",
        "print(f\"Standard diagnoses: {true_diagnosis_standard}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diagnoses_list = diagnoses_df['Diagnosticos_estandar'].tolist()\n",
        "diagnoses_list = list(set(code for sublist in diagnoses_list for code in sublist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_name = \"dmis-lab/biobert-v1.1\"\n",
        "# model_name = \"medicalai/ClinicalBERT\"\n",
        "# model_name = \"bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\"\n",
        "# model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\" ### PubMedBERT\n",
        "# model_name = \"PlanTL-GOB-ES/bsc-bio-es\"\n",
        "# model_name = \"yikuan8/Clinical-Longformer\"\n",
        "# model_name = \"yikuan8/Clinical-BigBird\"\n",
        "# model_name = \"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\"\n",
        "# model_name = \"PlanTL-GOB-ES/roberta-base-biomedical-es\"\n",
        "model_name = \"intfloat/multilingual-e5-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "max_tokens = min(tokenizer.model_max_length, AutoConfig.from_pretrained(model_name).max_position_embeddings)\n",
        "\n",
        "# Function to get the embedding of a sentence\n",
        "def get_embedding(text, max_len=512):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use the [CLS] token embedding as the sentence representation\n",
        "    return outputs.last_hidden_state[:, 0, :].cpu() \n",
        "\n",
        "# Get embeddings\n",
        "emb_diagnosticos = torch.vstack([get_embedding(d, max_tokens) for d in diagnoses_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get embeddings\n",
        "emb_desc = get_embedding(diagnosis_description, max_tokens)\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_sim = F.cosine_similarity(emb_desc, emb_diagnosticos)\n",
        "\n",
        "# Show results\n",
        "top_idx = torch.argmax(cos_sim).item()\n",
        "print(\"Closest diagnosis:\", diagnoses_list[top_idx])\n",
        "print(\"Similarity:\", cos_sim[top_idx].item())\n",
        "\n",
        "# Compute similarity with the true standard diagnoses\n",
        "true_diagnosis_embeddings = torch.vstack([get_embedding(d, max_tokens) for d in true_diagnosis_standard])\n",
        "true_cos_sim = F.cosine_similarity(emb_desc, true_diagnosis_embeddings)\n",
        "\n",
        "# Show results\n",
        "for idx, diagnosis in enumerate(true_diagnosis_standard):\n",
        "    print(f\"Similarity with '{diagnosis}': {true_cos_sim[idx].item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "correct_count = 0\n",
        "\n",
        "for idx, row in tqdm(diagnoses_df.iterrows(), total=len(diagnoses_df)):\n",
        "    # Get the diagnosis description\n",
        "    diagnosis_description = row[\"Descripcion_diagnosticos_limpio\"]\n",
        "    true_diagnosis_standard = row[\"Diagnosticos_estandar\"]\n",
        "\n",
        "    # Compute the embedding of the description\n",
        "    emb_desc = get_embedding(diagnosis_description, max_tokens)\n",
        "\n",
        "    # Compute cosine similarity with all diagnoses\n",
        "    cos_sim = F.cosine_similarity(emb_desc, emb_diagnosticos)\n",
        "\n",
        "    # Find the closest diagnosis\n",
        "    top_idx = torch.argmax(cos_sim).item()\n",
        "    predicted_diagnosis = diagnoses_list[top_idx]\n",
        "\n",
        "    # Check if the predicted diagnosis is in the true standard diagnoses\n",
        "    if predicted_diagnosis in true_diagnosis_standard:\n",
        "        correct_count += 1\n",
        "    \n",
        "# Calculate accuracy metric\n",
        "accuracy = correct_count / len(diagnoses_df)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{diagnosis_description=}\")\n",
        "print(f\"{true_diagnosis_standard=}\")\n",
        "print(f\"{cos_sim.sort(descending=True).values=}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
