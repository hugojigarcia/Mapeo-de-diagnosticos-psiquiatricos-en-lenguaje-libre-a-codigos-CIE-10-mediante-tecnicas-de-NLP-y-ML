{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Libraries \ud83d\udcda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import optuna\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load data \ud83d\udce5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diagnoses_df = pd.read_csv(\"data/ground_truth_df.csv\")\n",
        "for col in [\"Codigos_diagnosticos\", \"Diagnosticos_estandar\"]:\n",
        "    diagnoses_df[col] = diagnoses_df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "diagnoses_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pre-process and splits \ud83e\uddf9\u2702\ufe0f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(diagnoses_df[\"Diagnosticos_estandar\"])\n",
        "# Add the prefix required by e5-Large\n",
        "texts_prefixed = [\"query: \" + t for t in diagnoses_df[\"Descripcion_diagnosticos\"].tolist()]\n",
        "\n",
        "# Random splits on multilabel data, ensuring that each label's distribution is preserved across training and test sets.\n",
        "msss = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1, test_size=0.30, random_state=SEED\n",
        ")\n",
        "X = np.array(texts_prefixed)\n",
        "\n",
        "for train_idx, tmp_idx in msss.split(np.zeros(len(X)), y):\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_tmp,   y_tmp   = X[tmp_idx],   y[tmp_idx]\n",
        "\n",
        "# 50-50 over the 30 % left \u21d2 15 %/15 %\n",
        "msss_val = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1, test_size=0.50, random_state=SEED\n",
        ")\n",
        "for val_idx, test_idx in msss_val.split(np.zeros(len(X_tmp)), y_tmp):\n",
        "    X_val,  y_val  = X_tmp[val_idx],  y_tmp[val_idx]\n",
        "    X_test, y_test = X_tmp[test_idx], y_tmp[test_idx]\n",
        "\n",
        "# Convert to lists for compatibility with SentenceTransformer\n",
        "X_train, X_val, X_test = map(lambda a: a.tolist(), [X_train, X_val, X_test])\n",
        "\n",
        "# Convert to numpy arrays and float32 for PyTorch compatibility\n",
        "y_train, y_val, y_test = y_train.astype(np.float32), y_val.astype(np.float32), y_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check that each label maintains its ratio approx.\n",
        "train_ratio = y_train.sum(axis=0) / y.sum(axis=0)\n",
        "val_ratio   = y_val.sum(axis=0)   / y.sum(axis=0)\n",
        "test_ratio  = y_test.sum(axis=0)  / y.sum(axis=0)\n",
        "\n",
        "print(f\"train: {np.round(train_ratio.mean(), 3)}, val: {np.round(val_ratio.mean(), 3)}, test: {np.round(test_ratio.mean(), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset and model architecture \u2699\ufe0f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Load model and tokenizer ===\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# === Custom classifier model ===\n",
        "class DiagnosisClassifier(nn.Module):\n",
        "    def __init__(self, base_model, base_model_output_dim=1024, hidden_dim=768, num_labels=10):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(base_model_output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, num_labels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# === Dataset wrapper ===\n",
        "class DiagnosisDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline single model \ud83c\udf31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS_BASELINE = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "BATCH_SIZE = 16\n",
        "HIDDEN_DIM = 1024\n",
        "MAX_LEN = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = DiagnosisDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "val_ds   = DiagnosisDataset(X_val,   y_val,   tokenizer, MAX_LEN)\n",
        "test_ds = DiagnosisDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Load model and tokenizer ===\n",
        "base_model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# === Initialize model ===\n",
        "baseline = DiagnosisClassifier(base_model=base_model, hidden_dim=HIDDEN_DIM, num_labels=y.shape[1])\n",
        "baseline.to(device)\n",
        "\n",
        "# === Training setup ===\n",
        "optimizer = torch.optim.AdamW(baseline.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_loop(model, data_loader, criterion, optimizer, train=False):\n",
        "    model.train() if train else model.eval()\n",
        "    losses, logits_list, labels_list = [], [], []\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=f\"Batches ({'train' if train else 'eval'})\"):\n",
        "        input_ids  = batch[\"input_ids\"].to(device)\n",
        "        attn_mask  = batch[\"attention_mask\"].to(device)\n",
        "        labels     = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attn_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        if train:\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        logits_list.append(torch.sigmoid(outputs).detach().cpu())\n",
        "        labels_list.append(labels.detach().cpu())\n",
        "\n",
        "    y_pred = (torch.vstack(logits_list) > 0.5).int().numpy()\n",
        "    y_true = torch.vstack(labels_list).numpy()\n",
        "    return (np.mean(losses),\n",
        "            f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "            precision_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
        "            recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_f1 = 0\n",
        "for epoch in range(1, EPOCHS_BASELINE+1):\n",
        "    print(f\"\ud83d\udd39 Epoch {epoch:02d} / {EPOCHS_BASELINE}\")\n",
        "    train_loss, train_f1, _, _ = epoch_loop(baseline, train_dl, criterion, optimizer, train=True)\n",
        "    val_loss, val_f1, _, _     = epoch_loop(baseline, val_dl, criterion, optimizer, train=False)\n",
        "    print(f\"Epoch [{epoch:02d}]  train loss={train_loss:.4f} | val loss={val_loss:.4f}\")\n",
        "    print(f\"Epoch [{epoch:02d}]  train f1={train_f1:.4f} | val f1={val_f1:.4f}\")\n",
        "    best_f1 = max(best_f1, val_f1)\n",
        "\n",
        "_, f1_train, precision_train, recall_train = epoch_loop(baseline, train_dl, criterion, optimizer, train=False)\n",
        "_, f1_val, precision_val, recall_val = epoch_loop(baseline, val_dl, criterion, optimizer, train=False)\n",
        "_, f1_test, precision_test, recall_test = epoch_loop(baseline, test_dl, criterion, optimizer, train=False)\n",
        "print(f\"\ud83d\udd39 TRAIN: F1 {f1_train:.4f} / Precision {precision_train:.4f} / Recall {recall_train:.4f}\")\n",
        "print(f\"\ud83d\udd39 VAL:   F1 {f1_val:.4f} / Precision {precision_val:.4f} / Recall {recall_val:.4f}\")\n",
        "print(f\"\ud83d\udd39 TEST:  F1 {f1_test:.4f} / Precision {precision_test:.4f} / Recall {recall_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model with the best hyperparameters found by Optuna\n",
        "torch.save({\n",
        "    \"model_state\": baseline.state_dict(),\n",
        "    \"tokenizer\": MODEL_NAME,\n",
        "    \"mlb_classes\": mlb.classes_.tolist(),\n",
        "    \"params\": {\n",
        "        \"hidden_dim\": HIDDEN_DIM,\n",
        "        \"max_len\": MAX_LEN,\n",
        "        \"lr\": LEARNING_RATE,\n",
        "        \"epochs\": EPOCHS_BASELINE\n",
        "    }\n",
        "}, \"models/baseline_embeddings_and_nn.pt\")\n",
        "print(\"\ud83d\udce6 Baseline model saved as models/baseline_embeddings_and_nn.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del train_ds, val_ds, test_ds\n",
        "del train_dl, val_dl, test_dl\n",
        "del base_model, baseline\n",
        "del optimizer, criterion\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Hyperparameter Search with Optuna \ud83d\ude80\n",
        "-------------------------------------------------\n",
        "We search for LR, hidden_dim, max_len, and batch_size.  \n",
        "We use pruning to stop underperforming experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE_OPTUNA = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # --- Hyperparameters to explore ---\n",
        "    lr          = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
        "    hidden_dim  = trial.suggest_int(\"hidden_dim\", 256, 1536, step=64)\n",
        "    max_len     = trial.suggest_int(\"max_len\", 128, 256, step=64)\n",
        "    epochs      = trial.suggest_int(\"epochs\", 3, 20)\n",
        "\n",
        "    print(f\"\ud83d\udd0d Trial {trial.number} | lr={lr:.6f}, hidden_dim={hidden_dim}, max_len={max_len}, epochs={epochs}\")\n",
        "\n",
        "    # ---- Dataset - (re-tokenizo for the specific max_len) ----\n",
        "    token_tmp = tokenizer\n",
        "    train_ds_t = DiagnosisDataset(X_train, y_train, token_tmp, max_len)\n",
        "    val_ds_t   = DiagnosisDataset(X_val,   y_val,   token_tmp, max_len)\n",
        "    train_dl_t = DataLoader(train_ds_t, batch_size=BATCH_SIZE_OPTUNA, shuffle=True)\n",
        "    val_dl_t   = DataLoader(val_ds_t,   batch_size=BATCH_SIZE_OPTUNA)\n",
        "\n",
        "    base_model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "    for p in base_model.parameters(): p.requires_grad = True\n",
        "    model_t = DiagnosisClassifier(base_model=base_model, hidden_dim=hidden_dim, num_labels=y.shape[1]).to(device)\n",
        "\n",
        "    optimizer_t = torch.optim.AdamW(model_t.parameters(), lr=lr)\n",
        "    criterion_t = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # ---- Trained for a few epochs with pruning ----\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\ud83d\udd39 Trial {trial.number} - Epoch {epoch+1}/{epochs}\")\n",
        "        train_loss_epoch, train_f1_epoch, _, _ = epoch_loop(model_t, train_dl_t, criterion_t, optimizer_t, train=True)\n",
        "        val_loss_epoch, val_f1_epoch, _, _ = epoch_loop(model_t, val_dl_t, criterion_t, optimizer_t, train=False)\n",
        "        trial.report(val_f1_epoch, epoch)\n",
        "        print(f\"Epoch [{epoch:02d}]  train loss={train_loss_epoch:.4f} | val loss={val_loss_epoch:.4f}\")\n",
        "        print(f\"Epoch [{epoch:02d}]  train f1={train_f1_epoch:.4f} | val f1={val_f1_epoch:.4f}\")\n",
        "\n",
        "        if trial.should_prune(): raise optuna.TrialPruned()\n",
        "    \n",
        "    # ---- Delete everything to free memory ----\n",
        "    del train_ds_t, val_ds_t\n",
        "    del train_dl_t, val_dl_t\n",
        "    del base_model, model_t\n",
        "    del optimizer_t, criterion_t\n",
        "    gc.collect()\n",
        "\n",
        "    # ---- Return the final validation F1 score ----\n",
        "    return val_f1_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"diagnosis_cls_baseline\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "    storage=\"sqlite:///optuna/Embeddings_and_classification_layer.db\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "TOTAL_TRIALS = 18\n",
        "remaining_trials = max(TOTAL_TRIALS - len(study.trials), 0)\n",
        "study.optimize(objective, n_trials=remaining_trials, n_jobs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\u2705 Best trial:\", study.best_trial.number)\n",
        "print(\"\ud83c\udfc6 Best configuration:\", study.best_params)\n",
        "print(\"\ud83d\udd1d Best F1 val:\", study.best_value)\n",
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Retrain the model with the best configuration \ud83d\udd04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1) Retrieve the best search space ---\n",
        "\n",
        "best_params = study.best_params\n",
        "print(best_params)\n",
        "\n",
        "BEST_LR             = best_params[\"lr\"]\n",
        "BEST_HIDDEN         = best_params[\"hidden_dim\"]\n",
        "BEST_MAX_LEN        = best_params[\"max_len\"]\n",
        "BEST_EPOCHS         = best_params[\"epochs\"]\n",
        "\n",
        "# --- 2) Dataset: train + val (80 %) ---\n",
        "token_tmp = tokenizer\n",
        "train_ds_t = DiagnosisDataset(X_train, y_train, token_tmp, BEST_MAX_LEN)\n",
        "val_ds_t   = DiagnosisDataset(X_val,   y_val,   token_tmp, BEST_MAX_LEN)\n",
        "test_ds_t  = DiagnosisDataset(X_test, y_test, token_tmp, BEST_MAX_LEN)\n",
        "train_dl_t = DataLoader(train_ds_t, batch_size=BATCH_SIZE_OPTUNA, shuffle=True)\n",
        "val_dl_t   = DataLoader(val_ds_t,   batch_size=BATCH_SIZE_OPTUNA)\n",
        "test_dl_t  = DataLoader(test_ds_t,  batch_size=BATCH_SIZE_OPTUNA)\n",
        "\n",
        "# --- 3) Final model ---\n",
        "base_model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "for p in base_model.parameters(): p.requires_grad = True\n",
        "model_t = DiagnosisClassifier(base_model=base_model, hidden_dim=BEST_HIDDEN, num_labels=y.shape[1]).to(device)\n",
        "\n",
        "optimizer_t = torch.optim.AdamW(model_t.parameters(), lr=BEST_LR)\n",
        "criterion_t = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# --- 4) Training + early-stopping ---\n",
        "for epoch in range(BEST_EPOCHS):\n",
        "    print(f\"\ud83d\udd39 Epoch {epoch+1:02d} / {BEST_EPOCHS}\")\n",
        "    train_loss_epoch, train_f1_epoch, _, _ = epoch_loop(model_t, train_dl_t, criterion_t, optimizer_t, train=True)\n",
        "    val_loss_epoch, val_f1_epoch, _, _ = epoch_loop(model_t, val_dl_t, criterion_t, optimizer_t, train=False)\n",
        "    print(f\"Epoch [{epoch+1:02d}]  train loss={train_loss_epoch:.4f} | val loss={val_loss_epoch:.4f}\")\n",
        "    print(f\"Epoch [{epoch+1:02d}]  train f1={train_f1_epoch:.4f} | val f1={val_f1_epoch:.4f}\")\n",
        "    \n",
        "\n",
        "_, f1_train, precision_train, recall_train = epoch_loop(model_t, train_dl_t, criterion_t, optimizer_t, train=False)[1]\n",
        "_, f1_val, precision_val, recall_val = epoch_loop(model_t, val_dl_t, criterion_t, optimizer_t, train=False)[1]\n",
        "_, f1_test, precision_test, recall_test = epoch_loop(model_t, test_dl_t, criterion_t, optimizer_t, train=False)[1]\n",
        "print(f\"\ud83d\udd39 TRAIN: F1 {f1_train:.4f} / Precision {precision_train:.4f} / Recall {recall_train:.4f}\")\n",
        "print(f\"\ud83d\udd39 VAL:   F1 {f1_val:.4f} / Precision {precision_val:.4f} / Recall {recall_val:.4f}\")\n",
        "print(f\"\ud83d\udd39 TEST:  F1 {f1_test:.4f} / Precision {precision_test:.4f} / Recall {recall_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model with the best hyperparameters found by Optuna\n",
        "torch.save({\n",
        "    \"model_state\": model_t.state_dict(),\n",
        "    \"tokenizer\": MODEL_NAME,\n",
        "    \"mlb_classes\": mlb.classes_.tolist(),\n",
        "    \"params\": best_params\n",
        "}, \"models/optimized_embeddings_and_nn.pt\")\n",
        "print(\"\ud83d\udce6 Optuna best model saved as models/optimized_embeddings_and_nn.pt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
