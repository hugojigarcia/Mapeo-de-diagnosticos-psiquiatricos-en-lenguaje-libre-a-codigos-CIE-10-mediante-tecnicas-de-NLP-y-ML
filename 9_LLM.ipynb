{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "23dc9d57",
      "metadata": {},
      "source": [
        "## 0. Libraries ðŸ“š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29da659a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from utils import read_cie10_file\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "import numpy as np\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8dfad4",
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f89bb2",
      "metadata": {},
      "source": [
        "## 1. Load data ðŸ“¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53e18d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "diagnoses_df = pd.read_csv(\"data/ground_truth_df.csv\")\n",
        "diagnoses_df['Codigos_diagnosticos'] = diagnoses_df['Codigos_diagnosticos'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "diagnoses_df['Diagnosticos_estandar'] = diagnoses_df['Diagnosticos_estandar'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "diagnoses_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a2eb44",
      "metadata": {},
      "outputs": [],
      "source": [
        "cie10_map = read_cie10_file(\"data/diagnosticos_tipos.csv\")\n",
        "cie10_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1640f53",
      "metadata": {},
      "source": [
        "## 2. Pre-process and splits ðŸ§¹âœ‚ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db34f1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42  # reproducibility â”€ adapt if you already defined it\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1.  Features (X) â€“ cleaned text with the \"query: \" prefix\n",
        "# ------------------------------------------------------------------\n",
        "X = np.array(diagnoses_df[\"Descripcion_diagnosticos_limpio\"].tolist())\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.  Targets â€“ keep the *raw* lists\n",
        "# ------------------------------------------------------------------\n",
        "codes_lists = diagnoses_df[\"Codigos_diagnosticos\"].tolist()        # list[list[str]]\n",
        "std_lists   = diagnoses_df[\"Diagnosticos_estandar\"].tolist()       # list[list[str]]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3.  Temporary one-hot (only for the splitter)\n",
        "# ------------------------------------------------------------------\n",
        "mlb_codes = MultiLabelBinarizer()\n",
        "codes_enc = mlb_codes.fit_transform(codes_lists)\n",
        "\n",
        "mlb_std = MultiLabelBinarizer()\n",
        "std_enc  = mlb_std.fit_transform(std_lists)\n",
        "\n",
        "y_for_split = np.hstack([codes_enc, std_enc])  # shape: n_samples Ã— (n_codes + n_std)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.  First split  â†’ 70 % train  /  30 % temp\n",
        "# ------------------------------------------------------------------\n",
        "msss = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1, test_size=0.30, random_state=SEED\n",
        ")\n",
        "for train_idx, tmp_idx in msss.split(np.zeros(len(X)), y_for_split):\n",
        "    X_train = X[train_idx]\n",
        "    X_tmp   = X[tmp_idx]\n",
        "\n",
        "    # *** keep raw labels ***\n",
        "    codes_train = [codes_lists[i] for i in train_idx]\n",
        "    codes_tmp   = [codes_lists[i] for i in tmp_idx]\n",
        "\n",
        "    std_train   = [std_lists[i]   for i in train_idx]\n",
        "    std_tmp     = [std_lists[i]   for i in tmp_idx]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5.  Second split on the 30 % temp  â†’ 15 % val  /  15 % test\n",
        "# ------------------------------------------------------------------\n",
        "msss_val = MultilabelStratifiedShuffleSplit(\n",
        "    n_splits=1, test_size=0.50, random_state=SEED\n",
        ")\n",
        "for val_idx, test_idx in msss_val.split(\n",
        "    np.zeros(len(X_tmp)),\n",
        "    np.hstack([codes_enc[tmp_idx], std_enc[tmp_idx]]),   # need encoded labels again\n",
        "):\n",
        "    X_val,  X_test  = X_tmp[val_idx],  X_tmp[test_idx]\n",
        "\n",
        "    codes_val  = [codes_tmp[i] for i in val_idx]\n",
        "    codes_test = [codes_tmp[i] for i in test_idx]\n",
        "\n",
        "    std_val    = [std_tmp[i]   for i in val_idx]\n",
        "    std_test   = [std_tmp[i]   for i in test_idx]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6.  Convert X back to lists (if you need plain Python lists later)\n",
        "# ------------------------------------------------------------------\n",
        "X_train, X_val, X_test = map(lambda a: a.tolist(), [X_train, X_val, X_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdecb8f6",
      "metadata": {},
      "source": [
        "## 3. GPT format messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28225951",
      "metadata": {},
      "outputs": [],
      "source": [
        "codes_for_prompt = \"\"\n",
        "for code, description in cie10_map.items():\n",
        "    codes_for_prompt += f\"{code} {description}\\n\"\n",
        "print(codes_for_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a6d55a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def return_prompt(codes_for_prompt, description):\n",
        "    return f\"\"\"Task: read Spanish free-text clinical notes and return **at most five** ICD-10\n",
        "    codes with their official Spanish descriptions. Use only codes present in the\n",
        "    following list. If none apply, output [].\n",
        "    Think step by step internally but DO NOT reveal your reasoning.\n",
        "    Return a JSON array exactly like this:\n",
        "    [\n",
        "    {{\"code\":\"FXX.X\",\"label\":\"DescripciÃ³n oficial 1\"}},\n",
        "    {{\"code\":\"FYY.Y\",\"label\":\"DescripciÃ³n oficial 2\"}},\n",
        "    ...\n",
        "    ]\n",
        "    No extra keys, no text outside the JSON.\n",
        "\n",
        "    --- ALLOWED LIST (83 codes) ---\n",
        "    {codes_for_prompt}\n",
        "    --- END OF ALLOWED LIST ---\n",
        "\n",
        "    Clinical note:\n",
        "    {description}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4064dc41",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"train_examples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for description, codes, stds in zip(X_train, codes_train, std_train):\n",
        "        llm_result = []\n",
        "        for code, std in zip(codes, stds):\n",
        "            llm_result.append({\"code\": code, \"label\": std})\n",
        "        llm_result = str(llm_result).replace(\"'\", '\"')\n",
        "        message = {\n",
        "            \"messages\":[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a clinical coder specialising in ICD-10 mental-health (F00â€“F99).\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": return_prompt(codes_for_prompt, description)\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": llm_result\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        f.write(json.dumps(message, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88daeda9",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"val_examples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for description, codes, stds in zip(X_val, codes_val, std_val):\n",
        "        llm_result = []\n",
        "        for code, std in zip(codes, stds):\n",
        "            llm_result.append({\"code\": code, \"label\": std})\n",
        "        llm_result = str(llm_result).replace(\"'\", '\"')\n",
        "        message = {\n",
        "            \"messages\":[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a clinical coder specialising in ICD-10 mental-health (F00â€“F99).\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": return_prompt(codes_for_prompt, description)\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": llm_result\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        f.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e68640",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"test_examples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for description, codes, stds in zip(X_test, codes_test, std_test):\n",
        "        llm_result = []\n",
        "        for code, std in zip(codes, stds):\n",
        "            llm_result.append({\"code\": code, \"label\": std})\n",
        "        llm_result = str(llm_result).replace(\"'\", '\"')\n",
        "        message = {\n",
        "            \"messages\":[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a clinical coder specialising in ICD-10 mental-health (F00â€“F99).\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": return_prompt(codes_for_prompt, description)\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": llm_result\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        f.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b1e4c1",
      "metadata": {},
      "source": [
        "## 4. Study max tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73c2ef7",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Load the model\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_id)\n",
        "tok.pad_token = tok.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f5c738",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def n_tokens(text: str) -> int:\n",
        "    return len(tok(text, add_special_tokens=False)[\"input_ids\"])\n",
        "\n",
        "def count_tokens_in_jsonl(filename):\n",
        "    n_tokens_list = []\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            # Extrae el contenido del mensaje del usuario\n",
        "            user_msg = next(m[\"content\"] for m in record[\"messages\"] if m[\"role\"] == \"user\")\n",
        "            n_tokens_list.append(n_tokens(user_msg))\n",
        "    return n_tokens_list\n",
        "\n",
        "train_tokens = count_tokens_in_jsonl(\"train_examples.jsonl\")\n",
        "val_tokens = count_tokens_in_jsonl(\"val_examples.jsonl\")\n",
        "test_tokens = count_tokens_in_jsonl(\"test_examples.jsonl\")\n",
        "\n",
        "print(f\"Test:  min={min(test_tokens)}, max={max(test_tokens)}, mean={sum(test_tokens)/len(test_tokens):.2f}\")\n",
        "print(f\"Train: min={min(train_tokens)}, max={max(train_tokens)}, mean={sum(train_tokens)/len(train_tokens):.2f}\")\n",
        "print(f\"Val:   min={min(val_tokens)}, max={max(val_tokens)}, mean={sum(val_tokens)/len(val_tokens):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748d720a",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Count coverage of diagnoses with max tokens\n",
        "\n",
        "MAX_TOKENS = 2_048\n",
        "train_coverage = (np.array(train_tokens) <= MAX_TOKENS).mean()\n",
        "val_coverage = (np.array(val_tokens) <= MAX_TOKENS).mean()\n",
        "test_coverage = (np.array(test_tokens) <= MAX_TOKENS).mean()\n",
        "print(f\"Train coverage â‰¤{MAX_TOKENS}: {train_coverage*100:.2f}%\")\n",
        "print(f\"Val coverage â‰¤{MAX_TOKENS}: {val_coverage*100:.2f}%\")\n",
        "print(f\"Test coverage â‰¤{MAX_TOKENS}: {test_coverage*100:.2f}%\")\n",
        "print()\n",
        "\n",
        "total_tokens = np.array(train_tokens + val_tokens + test_tokens)\n",
        "total_coverage = (total_tokens <= MAX_TOKENS).mean()\n",
        "print(f\"Total coverage â‰¤{MAX_TOKENS}: {total_coverage*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be0cba5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "### View percentiles to document X% coverage\n",
        "pd.Series(total_tokens).describe(percentiles=[0.90, 0.95, 0.99, 0.997])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e79310",
      "metadata": {},
      "source": [
        "## 5. Fine tunning with PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb444f07",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "train_ds = load_dataset(\"json\", data_files=\"train_examples.jsonl\", split=\"train\")\n",
        "val_ds   = load_dataset(\"json\", data_files=\"val_examples.jsonl\", split=\"train\")\n",
        "\n",
        "print(train_ds[0][\"messages\"][0])\n",
        "print(train_ds[0][\"messages\"][1])\n",
        "print(train_ds[0][\"messages\"][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6d261b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5d0f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "# Forces the old mode of torch.load (allows to load rng_state.pth without errors)print(f â€œThe 10 most frequent diagnostics represent the {percentage_top_10:.2f}% of all diagnostics.â€)\n",
        "os.environ[\"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\"] = \"1\"\n",
        "\n",
        "# Hide only the warning related to that variable\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected.*\",\n",
        "    category=UserWarning\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edd9f03",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "output_dir = \"qlora_llama3B\"\n",
        "# output_dir = \"qlora_llama1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d17439",
      "metadata": {},
      "outputs": [],
      "source": [
        "## FINE-TUNE LLM WITH PEFT ###\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                             bnb_4bit_compute_dtype=\"bfloat16\",\n",
        "                             bnb_4bit_use_double_quant=True)\n",
        "\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_cfg,\n",
        "                                             device_map=\"auto\")\n",
        "\n",
        "lora_cfg = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "                      lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "\n",
        "def formatting(ex):\n",
        "    return tok.apply_chat_template(ex[\"messages\"], tokenize=False)\n",
        "\n",
        "sft_args = SFTConfig(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=1, # Paraliza. Valor mayor mÃ¡s memoria\n",
        "        gradient_accumulation_steps=16, # Cada cuanto actualiza los gradientes. \"Simula\" un batch mayor sin ocupar tanta memoria\n",
        "        num_train_epochs=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=100,\n",
        "        save_strategy=\"epoch\",\n",
        "        max_length=2048,\n",
        "        # eval_strategy=\"epoch\",\n",
        "        # per_device_eval_batch_size=1,\n",
        "        packing=True,\n",
        "        disable_tqdm=False)\n",
        "\n",
        "trainer = SFTTrainer(model=model,\n",
        "                     train_dataset=train_ds,\n",
        "                     eval_dataset=val_ds,\n",
        "                     peft_config=lora_cfg,\n",
        "                     formatting_func=formatting,\n",
        "                     args=sft_args)\n",
        "\n",
        "trainer.train(resume_from_checkpoint = True)\n",
        "trainer.save_model(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "191e12e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "### MERGE AND SAVE THE PEFT FINE-TUNED MODEL ###\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base and LoRA adapter\n",
        "base = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model = PeftModel.from_pretrained(base, output_dir)\n",
        "\n",
        "# Merge and unload LoRA into base\n",
        "merged = model.merge_and_unload()\n",
        "\n",
        "# Save merged model and tokenizer\n",
        "merged.save_pretrained(f\"{output_dir}_merged\")\n",
        "AutoTokenizer.from_pretrained(model_id).save_pretrained(f\"{output_dir}_merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b551c008",
      "metadata": {},
      "source": [
        "## 6. Evaluation with vLLM deployed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f47c40",
      "metadata": {},
      "outputs": [],
      "source": [
        "codes_for_prompt = \"\"\n",
        "for code, description in cie10_map.items():\n",
        "    codes_for_prompt += f\"{code} {description}\\n\"\n",
        "print(codes_for_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99df63a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def process_llm_output(response):\n",
        "    def process_llm_dic(llm_dict):\n",
        "        return process_llm_element([item for pair in llm_dict.items() for item in pair])\n",
        "    \n",
        "    def process_llm_list(llm_list):\n",
        "        result = []\n",
        "        for el in llm_list:\n",
        "            processed_el = process_llm_element(el)\n",
        "            if isinstance(processed_el, str):\n",
        "                result.append(processed_el)\n",
        "            else:\n",
        "                result.extend(processed_el)\n",
        "        return result\n",
        "    \n",
        "    def process_llm_element(llm_element):\n",
        "        if isinstance(llm_element, list):\n",
        "            return process_llm_list(llm_element)\n",
        "        elif isinstance(llm_element, dict):\n",
        "            return process_llm_dic(llm_element)\n",
        "        elif isinstance(llm_element, set):\n",
        "            return process_llm_list(list(llm_element))\n",
        "        elif isinstance(llm_element, int) or isinstance(llm_element, float):\n",
        "            return str(llm_element)\n",
        "        else:\n",
        "            return llm_element\n",
        "    \n",
        "    try:\n",
        "        data = json.loads(response.choices[0].message.content)\n",
        "        pred = process_llm_element(data)\n",
        "\n",
        "        pred = list(set(pred))  # Remove duplicates\n",
        "        pred = [item for item in pred if item in cie10_map.keys()]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pred = []\n",
        "        print(\"**************************\")\n",
        "        print(\"ERROR\")\n",
        "        try:\n",
        "            print(data)\n",
        "        except:\n",
        "            print(response)\n",
        "        print(\"**************************\")\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4427d897",
      "metadata": {},
      "source": [
        "### Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e17680f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, asyncio, nest_asyncio, httpx\n",
        "from typing import List, Dict\n",
        "from openai import AsyncOpenAI\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# MODEL_ID = \"qlora_llama3B_merged\"\n",
        "# MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# MODEL_ID = \"qlora_llama1B_merged\"\n",
        "\n",
        "CONCURRENCY = 8\n",
        "OUTPUT_FILE = f\"predictions/{MODEL_ID.replace('/', '_')}_predictions_train.jsonl\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"EMPTY\",\n",
        "    http_client=httpx.AsyncClient(verify=False),\n",
        ")\n",
        "sema = asyncio.Semaphore(CONCURRENCY)\n",
        "write_lock = asyncio.Lock()                # evita escrituras simultÃ¡neas\n",
        "\n",
        "def return_prompt(codes_for_prompt: str, description: str) -> str:\n",
        "    return f\"\"\"\n",
        "    You are a clinical coder specialising in ICD-10 mental-health (F00â€“F99).\n",
        "    Task: read Spanish free-text clinical notes and return **at most five** ICD-10\n",
        "    codes with their official Spanish descriptions. Use only codes present in the\n",
        "    following list. If none apply, output [].\n",
        "    Think step by step internally but DO NOT reveal your reasoning.\n",
        "    Return a JSON array exactly like this:\n",
        "    [\n",
        "      {{\"code\":\"FXX.X\",\"label\":\"DescripciÃ³n oficial\",\"score\":0.85}}\n",
        "    ]\n",
        "    No extra keys, no text outside the JSON.\n",
        "\n",
        "    --- ALLOWED LIST (83 codes) ---\n",
        "    {codes_for_prompt}\n",
        "    --- END OF ALLOWED LIST ---\n",
        "\n",
        "    Clinical note:\n",
        "    {description}\n",
        "    \"\"\"\n",
        "\n",
        "def load_progress() -> Dict[int, List[str]]:\n",
        "    \"\"\"Carga las predicciones ya guardadas.\"\"\"\n",
        "    done: Dict[int, List[str]] = {}\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                rec = json.loads(line)\n",
        "                done[rec[\"idx\"]] = rec[\"pred\"]\n",
        "    return done\n",
        "\n",
        "def append_record(idx: int, pred: List[str]) -> None:\n",
        "    \"\"\"AÃ±ade una lÃ­nea al fichero en formato JSONL.\"\"\"\n",
        "    with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"idx\": idx, \"pred\": pred}, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "async def classify(idx: int, description: str, codes: List[str]) -> List[str]:\n",
        "    async with sema:\n",
        "        prompt = return_prompt(codes_for_prompt, description)\n",
        "        response = await client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "        )\n",
        "        pred = process_llm_output(response)\n",
        "\n",
        "    # Guarda la predicciÃ³n de forma atÃ³mica\n",
        "    async with write_lock:\n",
        "        append_record(idx, pred)\n",
        "\n",
        "    return pred\n",
        "\n",
        "async def main() -> None:\n",
        "    done = load_progress()\n",
        "    missing_indices = [i for i in range(len(X_train)) if i not in done]\n",
        "\n",
        "    # Ejecuta solo los que faltan\n",
        "    tasks = [\n",
        "        classify(i, X_train[i], codes_train[i])\n",
        "        for i in missing_indices\n",
        "    ]\n",
        "\n",
        "    for _ in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Processing\"):\n",
        "        await _\n",
        "\n",
        "    # Si (re)hemos completado todo el conjunto, calcula la mÃ©trica\n",
        "    done = load_progress()\n",
        "    if len(done) == len(X_train):\n",
        "        mlb = MultiLabelBinarizer().fit(codes_train)   # o tu mlb_codes existente\n",
        "        y_true = mlb.transform(codes_train)\n",
        "        # Predicciones en orden original\n",
        "        y_pred = mlb.transform([done[i] for i in range(len(X_train))])\n",
        "\n",
        "        # Metrics\n",
        "        f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        precision_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        recall_micro = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "\n",
        "        # Pretty print\n",
        "        print(\"\\n===== Train Metrics =====\")\n",
        "        print(f\"F1        : {f1_micro:.4f}\")\n",
        "        print(f\"Precision : {precision_micro:.4f}\")\n",
        "        print(f\"Recall    : {recall_micro:.4f}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint guardado: {len(done)}/{len(X_train)} ejemplos procesados.\")\n",
        "\n",
        "# â”€â”€ Celda de Jupyter â”€â”€\n",
        "nest_asyncio.apply()\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ffe236",
      "metadata": {},
      "source": [
        "### Val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a38ab1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, asyncio, nest_asyncio, httpx\n",
        "from typing import List, Dict\n",
        "from openai import AsyncOpenAI\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# MODEL_ID = \"qlora_llama3B_merged\"\n",
        "# MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# MODEL_ID = \"qlora_llama1B_merged\"\n",
        "\n",
        "CONCURRENCY = 8\n",
        "OUTPUT_FILE = f\"predictions/{MODEL_ID.replace('/', '_')}_predictions_val.jsonl\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"EMPTY\",\n",
        "    http_client=httpx.AsyncClient(verify=False),\n",
        ")\n",
        "sema = asyncio.Semaphore(CONCURRENCY)\n",
        "write_lock = asyncio.Lock()                # evita escrituras simultÃ¡neas\n",
        "\n",
        "def return_prompt(codes_for_prompt: str, description: str) -> str:\n",
        "    return f\"\"\"\n",
        "    You are a clinical coder specialising in ICD-10 mental-health (F00â€“F99).\n",
        "    Task: read Spanish free-text clinical notes and return **at most five** ICD-10\n",
        "    codes with their official Spanish descriptions. Use only codes present in the\n",
        "    following list. If none apply, output [].\n",
        "    Think step by step internally but DO NOT reveal your reasoning.\n",
        "    Return a JSON array exactly like this:\n",
        "    [\n",
        "      {{\"code\":\"FXX.X\",\"label\":\"DescripciÃ³n oficial\",\"score\":0.85}}\n",
        "    ]\n",
        "    No extra keys, no text outside the JSON.\n",
        "\n",
        "    --- ALLOWED LIST (83 codes) ---\n",
        "    {codes_for_prompt}\n",
        "    --- END OF ALLOWED LIST ---\n",
        "\n",
        "    Clinical note:\n",
        "    {description}\n",
        "    \"\"\"\n",
        "\n",
        "def load_progress() -> Dict[int, List[str]]:\n",
        "    \"\"\"Carga las predicciones ya guardadas.\"\"\"\n",
        "    done: Dict[int, List[str]] = {}\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                rec = json.loads(line)\n",
        "                done[rec[\"idx\"]] = rec[\"pred\"]\n",
        "    return done\n",
        "\n",
        "def append_record(idx: int, pred: List[str]) -> None:\n",
        "    \"\"\"AÃ±ade una lÃ­nea al fichero en formato JSONL.\"\"\"\n",
        "    with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"idx\": idx, \"pred\": pred}, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "async def classify(idx: int, description: str, codes: List[str]) -> List[str]:\n",
        "    async with sema:\n",
        "        prompt = return_prompt(codes_for_prompt, description)\n",
        "        response = await client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "        )\n",
        "        pred = process_llm_output(response)\n",
        "\n",
        "    # Guarda la predicciÃ³n de forma atÃ³mica\n",
        "    async with write_lock:\n",
        "        append_record(idx, pred)\n",
        "\n",
        "    return pred\n",
        "\n",
        "async def main() -> None:\n",
        "    done = load_progress()\n",
        "    missing_indices = [i for i in range(len(X_val)) if i not in done]\n",
        "\n",
        "    # Ejecuta solo los que faltan\n",
        "    tasks = [\n",
        "        classify(i, X_val[i], codes_val[i])\n",
        "        for i in missing_indices\n",
        "    ]\n",
        "\n",
        "    for _ in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Processing\"):\n",
        "        await _\n",
        "\n",
        "    # Si (re)hemos completado todo el conjunto, calcula la mÃ©trica\n",
        "    done = load_progress()\n",
        "    if len(done) == len(X_val):\n",
        "        mlb = MultiLabelBinarizer().fit(codes_val)   # o tu mlb_codes existente\n",
        "        y_true = mlb.transform(codes_val)\n",
        "        # Predicciones en orden original\n",
        "        y_pred = mlb.transform([done[i] for i in range(len(X_val))])\n",
        "\n",
        "        # Metrics\n",
        "        f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        precision_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        recall_micro = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "\n",
        "        # Pretty print\n",
        "        print(\"\\n===== Val Metrics =====\")\n",
        "        print(f\"F1        : {f1_micro:.4f}\")\n",
        "        print(f\"Precision : {precision_micro:.4f}\")\n",
        "        print(f\"Recall    : {recall_micro:.4f}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint guardado: {len(done)}/{len(X_val)} ejemplos procesados.\")\n",
        "\n",
        "# â”€â”€ Celda de Jupyter â”€â”€\n",
        "nest_asyncio.apply()\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d7255d",
      "metadata": {},
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a31bb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, asyncio, nest_asyncio, httpx\n",
        "from typing import List, Dict\n",
        "from openai import AsyncOpenAI\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# MODEL_ID = \"qlora_llama3B_merged\"\n",
        "# MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# MODEL_ID = \"qlora_llama1B_merged\"\n",
        "\n",
        "CONCURRENCY = 8\n",
        "OUTPUT_FILE = f\"predictions/{MODEL_ID.replace('/', '_')}_predictions_test.jsonl\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"EMPTY\",\n",
        "    http_client=httpx.AsyncClient(verify=False),\n",
        ")\n",
        "sema = asyncio.Semaphore(CONCURRENCY)\n",
        "write_lock = asyncio.Lock()                # evita escrituras simultÃ¡neas\n",
        "\n",
        "def return_prompt(codes_for_prompt: str, description: str) -> str:\n",
        "    return f\"\"\"\n",
        "    You are a clinical coder specialising in ICD-10 mental-health (F00â€“F99).\n",
        "    Task: read Spanish free-text clinical notes and return **at most five** ICD-10\n",
        "    codes with their official Spanish descriptions. Use only codes present in the\n",
        "    following list. If none apply, output [].\n",
        "    Think step by step internally but DO NOT reveal your reasoning.\n",
        "    Return a JSON array exactly like this:\n",
        "    [\n",
        "      {{\"code\":\"FXX.X\",\"label\":\"DescripciÃ³n oficial\",\"score\":0.85}}\n",
        "    ]\n",
        "    No extra keys, no text outside the JSON.\n",
        "\n",
        "    --- ALLOWED LIST (83 codes) ---\n",
        "    {codes_for_prompt}\n",
        "    --- END OF ALLOWED LIST ---\n",
        "\n",
        "    Clinical note:\n",
        "    {description}\n",
        "    \"\"\"\n",
        "\n",
        "def load_progress() -> Dict[int, List[str]]:\n",
        "    \"\"\"Carga las predicciones ya guardadas.\"\"\"\n",
        "    done: Dict[int, List[str]] = {}\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                rec = json.loads(line)\n",
        "                done[rec[\"idx\"]] = rec[\"pred\"]\n",
        "    return done\n",
        "\n",
        "def append_record(idx: int, pred: List[str]) -> None:\n",
        "    \"\"\"AÃ±ade una lÃ­nea al fichero en formato JSONL.\"\"\"\n",
        "    with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"idx\": idx, \"pred\": pred}, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "async def classify(idx: int, description: str, codes: List[str]) -> List[str]:\n",
        "    async with sema:\n",
        "        prompt = return_prompt(codes_for_prompt, description)\n",
        "        response = await client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "        )\n",
        "        pred = process_llm_output(response)\n",
        "\n",
        "    # Guarda la predicciÃ³n de forma atÃ³mica\n",
        "    async with write_lock:\n",
        "        append_record(idx, pred)\n",
        "\n",
        "    return pred\n",
        "\n",
        "async def main() -> None:\n",
        "    done = load_progress()\n",
        "    missing_indices = [i for i in range(len(X_test)) if i not in done]\n",
        "\n",
        "    # Ejecuta solo los que faltan\n",
        "    tasks = [\n",
        "        classify(i, X_test[i], codes_test[i])\n",
        "        for i in missing_indices\n",
        "    ]\n",
        "\n",
        "    for _ in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Processing\"):\n",
        "        await _\n",
        "\n",
        "    # Si (re)hemos completado todo el conjunto, calcula la mÃ©trica\n",
        "    done = load_progress()\n",
        "    if len(done) == len(X_test):\n",
        "        mlb = MultiLabelBinarizer().fit(codes_test)   # o tu mlb_codes existente\n",
        "        y_true = mlb.transform(codes_test)\n",
        "        # Predicciones en orden original\n",
        "        y_pred = mlb.transform([done[i] for i in range(len(X_test))])\n",
        "\n",
        "        # Metrics\n",
        "        f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        precision_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "        recall_micro = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "\n",
        "        # Pretty print\n",
        "        print(\"\\n===== Test Metrics =====\")\n",
        "        print(f\"F1        : {f1_micro:.4f}\")\n",
        "        print(f\"Precision : {precision_micro:.4f}\")\n",
        "        print(f\"Recall    : {recall_micro:.4f}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint guardado: {len(done)}/{len(X_test)} ejemplos procesados.\")\n",
        "\n",
        "# â”€â”€ Celda de Jupyter â”€â”€\n",
        "nest_asyncio.apply()\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
